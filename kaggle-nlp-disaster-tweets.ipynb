{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-22T12:37:05.610619Z","iopub.execute_input":"2023-08-22T12:37:05.611333Z","iopub.status.idle":"2023-08-22T12:37:05.622158Z","shell.execute_reply.started":"2023-08-22T12:37:05.611297Z","shell.execute_reply":"2023-08-22T12:37:05.621288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Importing and checking data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/nlpgettingstarted/train.csv')\ntest_df = pd.read_csv('/kaggle/input/nlpgettingstarted/test.csv')\nsample_sub_df = pd.read_csv('/kaggle/input/nlpgettingstarted/sample_submission.csv')\n\ntrain_df.head() # column: 'keyword' and 'location' are a bit blank so those columns will be left out for now.\ntrain_clean_df = train_df.drop(['keyword', 'location'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-08-22T13:05:00.299254Z","iopub.execute_input":"2023-08-22T13:05:00.299665Z","iopub.status.idle":"2023-08-22T13:05:00.363968Z","shell.execute_reply.started":"2023-08-22T13:05:00.299632Z","shell.execute_reply":"2023-08-22T13:05:00.363063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()\ntest_clean_df = test_df.drop(['keyword', 'location'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-08-22T13:05:09.817460Z","iopub.execute_input":"2023-08-22T13:05:09.817843Z","iopub.status.idle":"2023-08-22T13:05:09.824142Z","shell.execute_reply.started":"2023-08-22T13:05:09.817812Z","shell.execute_reply":"2023-08-22T13:05:09.823073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data cleaning process: For training data only the words should be existing in the text, no special characters (@,#,&...)","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport re\nimport string\n\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\nstop_words = set(stopwords.words('english'))\n\ndef remove_stopwords(text):\n    word_tokens = word_tokenize(text)\n    filtered_text = [word for word in word_tokens if word not in stop_words]\n    return ' '.join(filtered_text)  # Join the words back into a string\n\ntrain_clean_df['text'] = train_clean_df['text'].apply(lambda x: clean_text(x))\ntrain_clean_df['text'] = train_clean_df['text'].apply(lambda x: remove_stopwords(x))\n\ntest_clean_df['text'] = test_clean_df['text'].apply(lambda x: clean_text(x))\ntest_clean_df['text'] = test_clean_df['text'].apply(lambda x: remove_stopwords(x))","metadata":{"execution":{"iopub.status.busy":"2023-08-22T13:05:49.410706Z","iopub.execute_input":"2023-08-22T13:05:49.411070Z","iopub.status.idle":"2023-08-22T13:05:52.102793Z","shell.execute_reply.started":"2023-08-22T13:05:49.411041Z","shell.execute_reply":"2023-08-22T13:05:52.101818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_clean_df[train_clean_df.target == 0]\n\n# \"saving\" the cleaned dataset by copying\ntrain_df = train_clean_df.copy()\ntest_df = test_clean_df.copy()\n\ntrain_df.target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-08-22T13:15:14.306083Z","iopub.execute_input":"2023-08-22T13:15:14.306588Z","iopub.status.idle":"2023-08-22T13:15:14.330520Z","shell.execute_reply.started":"2023-08-22T13:15:14.306549Z","shell.execute_reply":"2023-08-22T13:15:14.328641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading pretrained BERT model \n\nThere are some pretrained BERT models, uncased is loaded because lower and uppercase letters are not needed to differentiate upper cased letter words and lower cased ones and large for better accuracy.","metadata":{}},{"cell_type":"code","source":"import transformers\n\nfrom transformers import AutoTokenizer,TFBertModel\ntokenizer = AutoTokenizer.from_pretrained('bert-large-uncased')\nbert = TFBertModel.from_pretrained('bert-large-uncased')","metadata":{"execution":{"iopub.status.busy":"2023-08-22T16:17:45.264947Z","iopub.execute_input":"2023-08-22T16:17:45.265526Z","iopub.status.idle":"2023-08-22T16:17:52.249181Z","shell.execute_reply.started":"2023-08-22T16:17:45.265482Z","shell.execute_reply":"2023-08-22T16:17:52.248256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting 'text' data to tokens for BERT\n\nprint(\"max text lenght:\",max([len(x.split()) for x in train_df.text])) # split words by space\n","metadata":{"execution":{"iopub.status.busy":"2023-08-22T16:17:58.096449Z","iopub.execute_input":"2023-08-22T16:17:58.096856Z","iopub.status.idle":"2023-08-22T16:17:58.112220Z","shell.execute_reply.started":"2023-08-22T16:17:58.096806Z","shell.execute_reply":"2023-08-22T16:17:58.111254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating input train data:\n# tokenizing and creating a tensor shape input\n\nx_train = tokenizer(\n    text=train_df.text.tolist(),\n    add_special_tokens=True,\n    max_length=23,\n    truncation=True, # if length is bigger then truncating the text\n    padding=True, # every text length be the same by padding\n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n\nx_train","metadata":{"execution":{"iopub.status.busy":"2023-08-22T16:18:00.729975Z","iopub.execute_input":"2023-08-22T16:18:00.730962Z","iopub.status.idle":"2023-08-22T16:18:01.156631Z","shell.execute_reply.started":"2023-08-22T16:18:00.730924Z","shell.execute_reply":"2023-08-22T16:18:01.155502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train['input_ids'].shape","metadata":{"execution":{"iopub.status.busy":"2023-08-22T16:18:04.094589Z","iopub.execute_input":"2023-08-22T16:18:04.095190Z","iopub.status.idle":"2023-08-22T16:18:04.103005Z","shell.execute_reply.started":"2023-08-22T16:18:04.095153Z","shell.execute_reply":"2023-08-22T16:18:04.101932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating output train data:\ny_train = train_df.target.values\ny_train","metadata":{"execution":{"iopub.status.busy":"2023-08-22T16:18:05.595142Z","iopub.execute_input":"2023-08-22T16:18:05.595527Z","iopub.status.idle":"2023-08-22T16:18:05.602628Z","shell.execute_reply.started":"2023-08-22T16:18:05.595496Z","shell.execute_reply":"2023-08-22T16:18:05.601473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Building a model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.optimizers.legacy import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy,BinaryCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy,BinaryAccuracy\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.utils import plot_model","metadata":{"execution":{"iopub.status.busy":"2023-08-22T16:18:07.809977Z","iopub.execute_input":"2023-08-22T16:18:07.810999Z","iopub.status.idle":"2023-08-22T16:18:07.818149Z","shell.execute_reply.started":"2023-08-22T16:18:07.810959Z","shell.execute_reply":"2023-08-22T16:18:07.816990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 23\ninput_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\ninput_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n\n# loading the BERT model:\nembeddings = bert(input_ids,attention_mask = input_mask)[1]\n\nout = tf.keras.layers.Dropout(0.2)(embeddings)\n\nout = Dense(128, activation='relu')(out)\nout = tf.keras.layers.Dropout(0.1)(out)\nout = Dense(32,activation = 'relu')(out)\n\ny = Dense(1,activation = 'sigmoid')(out)\n    \nmodel = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\nmodel.layers[2].trainable = True","metadata":{"execution":{"iopub.status.busy":"2023-08-22T16:18:10.217183Z","iopub.execute_input":"2023-08-22T16:18:10.217552Z","iopub.status.idle":"2023-08-22T16:18:13.991525Z","shell.execute_reply.started":"2023-08-22T16:18:10.217521Z","shell.execute_reply":"2023-08-22T16:18:13.990505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-22T16:18:15.914945Z","iopub.execute_input":"2023-08-22T16:18:15.915345Z","iopub.status.idle":"2023-08-22T16:18:15.994463Z","shell.execute_reply.started":"2023-08-22T16:18:15.915311Z","shell.execute_reply":"2023-08-22T16:18:15.993487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = Adam(\n    learning_rate=7e-06, # base value: https://huggingface.co/bert-large-uncased\n    epsilon=2e-08,\n    decay=0.01,\n    clipnorm=1.0)\n\n# Set loss and metrics\nloss = BinaryCrossentropy()\nmetric = BinaryAccuracy('accuracy'),\n# Compile the model\nmodel.compile(\n    optimizer = optimizer,\n    loss = loss, \n    metrics = metric)","metadata":{"execution":{"iopub.status.busy":"2023-08-22T16:22:39.185322Z","iopub.execute_input":"2023-08-22T16:22:39.185710Z","iopub.status.idle":"2023-08-22T16:22:39.215483Z","shell.execute_reply.started":"2023-08-22T16:22:39.185675Z","shell.execute_reply":"2023-08-22T16:22:39.214553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model, show_shapes = True)","metadata":{"execution":{"iopub.status.busy":"2023-08-22T16:22:42.396668Z","iopub.execute_input":"2023-08-22T16:22:42.397062Z","iopub.status.idle":"2023-08-22T16:22:42.573031Z","shell.execute_reply.started":"2023-08-22T16:22:42.397029Z","shell.execute_reply":"2023-08-22T16:22:42.572080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.config.experimental.list_physical_devices('GPU')","metadata":{"execution":{"iopub.status.busy":"2023-08-22T16:22:44.813777Z","iopub.execute_input":"2023-08-22T16:22:44.814179Z","iopub.status.idle":"2023-08-22T16:22:44.821084Z","shell.execute_reply.started":"2023-08-22T16:22:44.814147Z","shell.execute_reply":"2023-08-22T16:22:44.819984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training the model","metadata":{}},{"cell_type":"code","source":"train_history = model.fit(\n    x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,\n    y = y_train,\n    validation_split = 0.1,\n    epochs=10,\n    batch_size=32\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-22T16:23:21.516029Z","iopub.execute_input":"2023-08-22T16:23:21.517361Z","iopub.status.idle":"2023-08-22T16:45:29.436431Z","shell.execute_reply.started":"2023-08-22T16:23:21.517314Z","shell.execute_reply":"2023-08-22T16:45:29.435251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Testing:","metadata":{}},{"cell_type":"code","source":"test_df","metadata":{"execution":{"iopub.status.busy":"2023-08-22T16:46:59.203904Z","iopub.execute_input":"2023-08-22T16:46:59.208352Z","iopub.status.idle":"2023-08-22T16:46:59.247396Z","shell.execute_reply.started":"2023-08-22T16:46:59.208294Z","shell.execute_reply":"2023-08-22T16:46:59.246173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test input tokenizer:\n\nx_test = tokenizer(\n    text=test_df.text.tolist(),\n    add_special_tokens=True,\n    max_length=23,\n    truncation=True, # if length is bigger then truncating the text\n    padding=True, # every text length be the same by padding\n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)","metadata":{"execution":{"iopub.status.busy":"2023-08-22T16:47:45.357787Z","iopub.execute_input":"2023-08-22T16:47:45.358217Z","iopub.status.idle":"2023-08-22T16:47:45.558256Z","shell.execute_reply.started":"2023-08-22T16:47:45.358181Z","shell.execute_reply":"2023-08-22T16:47:45.556959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = model.predict({'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']})","metadata":{"execution":{"iopub.status.busy":"2023-08-22T16:47:48.280768Z","iopub.execute_input":"2023-08-22T16:47:48.281384Z","iopub.status.idle":"2023-08-22T16:48:14.697366Z","shell.execute_reply.started":"2023-08-22T16:47:48.281347Z","shell.execute_reply":"2023-08-22T16:48:14.696248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predicted = np.where(predicted>0.5,1,0)","metadata":{"execution":{"iopub.status.busy":"2023-08-22T16:53:16.602578Z","iopub.execute_input":"2023-08-22T16:53:16.603031Z","iopub.status.idle":"2023-08-22T16:53:16.608568Z","shell.execute_reply.started":"2023-08-22T16:53:16.602996Z","shell.execute_reply":"2023-08-22T16:53:16.607510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predicted = y_predicted.reshape((1,3263))[0]","metadata":{"execution":{"iopub.status.busy":"2023-08-22T16:53:19.032767Z","iopub.execute_input":"2023-08-22T16:53:19.033236Z","iopub.status.idle":"2023-08-22T16:53:19.039244Z","shell.execute_reply.started":"2023-08-22T16:53:19.033198Z","shell.execute_reply":"2023-08-22T16:53:19.037725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub_df['id'] = test_df.id\nsample_sub_df['target'] = y_predicted","metadata":{"execution":{"iopub.status.busy":"2023-08-22T16:53:21.487547Z","iopub.execute_input":"2023-08-22T16:53:21.487927Z","iopub.status.idle":"2023-08-22T16:53:21.493573Z","shell.execute_reply.started":"2023-08-22T16:53:21.487895Z","shell.execute_reply":"2023-08-22T16:53:21.492558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub_df.to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2023-08-22T16:53:22.969734Z","iopub.execute_input":"2023-08-22T16:53:22.970889Z","iopub.status.idle":"2023-08-22T16:53:22.985347Z","shell.execute_reply.started":"2023-08-22T16:53:22.970839Z","shell.execute_reply":"2023-08-22T16:53:22.984168Z"},"trusted":true},"execution_count":null,"outputs":[]}]}